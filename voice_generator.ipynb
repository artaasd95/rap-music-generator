{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Voice Generator Model Training\n",
       "\n",
       "This notebook implements a voice generator model using Tacotron2 for text-to-speech synthesis and WaveNet for audio generation."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import torch\n",
       "import torchaudio\n",
       "import torch.nn as nn\n",
       "import torch.nn.functional as F\n",
       "from torch.utils.data import Dataset, DataLoader\n",
       "import numpy as np\n",
       "import librosa\n",
       "import os\n",
       "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
       "import matplotlib.pyplot as plt\n",
       "from tqdm import tqdm"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load the trained rap lyrics generator model\n",
       "model_path = './trained_model'\n",
       "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
       "lyrics_model = AutoModelForCausalLM.from_pretrained(model_path)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "class RapVoiceDataset(Dataset):\n",
       "    def __init__(self, audio_dir, lyrics_file, tokenizer, max_audio_length=16000*10):\n",
       "        self.audio_dir = audio_dir\n",
       "        self.tokenizer = tokenizer\n",
       "        self.max_audio_length = max_audio_length\n",
       "        \n",
       "        # Load audio files and their corresponding lyrics\n",
       "        self.audio_files = []\n",
       "        self.lyrics = []\n",
       "        \n",
       "        # TODO: Implement loading of audio files and lyrics\n",
       "        \n",
       "    def __len__(self):\n",
       "        return len(self.audio_files)\n",
       "    \n",
       "    def __getitem__(self, idx):\n",
       "        audio_path = self.audio_files[idx]\n",
       "        lyrics = self.lyrics[idx]\n",
       "        \n",
       "        # Load and preprocess audio\n",
       "        audio, sr = torchaudio.load(audio_path)\n",
       "        if audio.shape[0] > 1:  # Convert stereo to mono\n",
       "            audio = audio.mean(dim=0, keepdim=True)\n",
       "        \n",
       "        # Resample if necessary\n",
       "        if sr != 22050:\n",
       "            resampler = torchaudio.transforms.Resample(sr, 22050)\n",
       "            audio = resampler(audio)\n",
       "        \n",
       "        # Trim or pad audio to max_audio_length\n",
       "        if audio.shape[1] > self.max_audio_length:\n",
       "            audio = audio[:, :self.max_audio_length]\n",
       "        else:\n",
       "            padding = self.max_audio_length - audio.shape[1]\n",
       "            audio = F.pad(audio, (0, padding))\n",
       "        \n",
       "        # Tokenize lyrics\n",
       "        lyrics_tokens = self.tokenizer.encode(lyrics, return_tensors='pt')\n",
       "        \n",
       "        return {\n",
       "            'audio': audio,\n",
       "            'lyrics': lyrics_tokens\n",
       "        }"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "class Tacotron2(nn.Module):\n",
       "    def __init__(self, vocab_size, embedding_dim=512, encoder_dim=512, decoder_dim=1024):\n",
       "        super().__init__()\n",
       "        \n",
       "        # Text encoder\n",
       "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
       "        self.encoder = nn.Sequential(\n",
       "            nn.Conv1d(embedding_dim, encoder_dim, kernel_size=3, padding=1),\n",
       "            nn.BatchNorm1d(encoder_dim),\n",
       "            nn.ReLU(),\n",
       "            nn.Conv1d(encoder_dim, encoder_dim, kernel_size=3, padding=1),\n",
       "            nn.BatchNorm1d(encoder_dim),\n",
       "            nn.ReLU()\n",
       "        )\n",
       "        \n",
       "        # Decoder\n",
       "        self.decoder = nn.GRU(\n",
       "            input_size=encoder_dim,\n",
       "            hidden_size=decoder_dim,\n",
       "            num_layers=2,\n",
       "            batch_first=True\n",
       "        )\n",
       "        \n",
       "        # Mel spectrogram prediction\n",
       "        self.mel_predictor = nn.Sequential(\n",
       "            nn.Linear(decoder_dim, 80),  # 80 mel bands\n",
       "            nn.Tanh()\n",
       "        )\n",
       "        \n",
       "    def forward(self, x):\n",
       "        # x: [batch_size, seq_len]\n",
       "        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
       "        embedded = embedded.transpose(1, 2)  # [batch_size, embedding_dim, seq_len]\n",
       "        \n",
       "        # Encode\n",
       "        encoded = self.encoder(embedded)  # [batch_size, encoder_dim, seq_len]\n",
       "        encoded = encoded.transpose(1, 2)  # [batch_size, seq_len, encoder_dim]\n",
       "        \n",
       "        # Decode\n",
       "        decoded, _ = self.decoder(encoded)  # [batch_size, seq_len, decoder_dim]\n",
       "        \n",
       "        # Predict mel spectrogram\n",
       "        mel_spec = self.mel_predictor(decoded)  # [batch_size, seq_len, 80]\n",
       "        \n",
       "        return mel_spec"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "class WaveNet(nn.Module):\n",
       "    def __init__(self, in_channels=80, out_channels=1, layers=20, residual_channels=64, gate_channels=64, skip_channels=64):\n",
       "        super().__init__()\n",
       "        \n",
       "        self.in_channels = in_channels\n",
       "        self.out_channels = out_channels\n",
       "        self.layers = layers\n",
       "        self.residual_channels = residual_channels\n",
       "        self.gate_channels = gate_channels\n",
       "        self.skip_channels = skip_channels\n",
       "        \n",
       "        # Initial convolution\n",
       "        self.start_conv = nn.Conv1d(in_channels, residual_channels, 1)\n",
       "        \n",
       "        # Dilated convolutions\n",
       "        self.dilated_convs = nn.ModuleList()\n",
       "        self.gate_convs = nn.ModuleList()\n",
       "        self.skip_convs = nn.ModuleList()\n",
       "        self.residual_convs = nn.ModuleList()\n",
       "        \n",
       "        for layer in range(layers):\n",
       "            dilation = 2 ** layer\n",
       "            padding = (3 - 1) * dilation // 2\n",
       "            \n",
       "            self.dilated_convs.append(\n",
       "                nn.Conv1d(residual_channels, gate_channels, 3, padding=padding, dilation=dilation)\n",
       "            )\n",
       "            self.gate_convs.append(\n",
       "                nn.Conv1d(residual_channels, gate_channels, 3, padding=padding, dilation=dilation)\n",
       "            )\n",
       "            self.skip_convs.append(nn.Conv1d(gate_channels, skip_channels, 1))\n",
       "            self.residual_convs.append(nn.Conv1d(gate_channels, residual_channels, 1))\n",
       "        \n",
       "        # Final convolutions\n",
       "        self.final_conv = nn.Sequential(\n",
       "            nn.ReLU(),\n",
       "            nn.Conv1d(skip_channels, skip_channels, 1),\n",
       "            nn.ReLU(),\n",
       "            nn.Conv1d(skip_channels, out_channels, 1),\n",
       "            nn.Tanh()\n",
       "        )\n",
       "        \n",
       "    def forward(self, x):\n",
       "        # x: [batch_size, in_channels, time]\n",
       "        x = self.start_conv(x)\n",
       "        skip = 0\n",
       "        \n",
       "        for i in range(self.layers):\n",
       "            residual = x\n",
       "            \n",
       "            # Dilated convolution\n",
       "            x = self.dilated_convs[i](x)\n",
       "            g = self.gate_convs[i](residual)\n",
       "            \n",
       "            # Gated activation\n",
       "            x = torch.tanh(x) * torch.sigmoid(g)\n",
       "            \n",
       "            # Skip connection\n",
       "            skip = skip + self.skip_convs[i](x)\n",
       "            \n",
       "            # Residual connection\n",
       "            x = self.residual_convs[i](x) + residual\n",
       "        \n",
       "        # Final convolution\n",
       "        x = self.final_conv(skip)\n",
       "        \n",
       "        return x"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def train_voice_generator(model, train_loader, optimizer, device, epochs=10):\n",
       "    model.train()\n",
       "    \n",
       "    for epoch in range(epochs):\n",
       "        total_loss = 0\n",
       "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
       "        \n",
       "        for batch in progress_bar:\n",
       "            audio = batch['audio'].to(device)\n",
       "            lyrics = batch['lyrics'].to(device)\n",
       "            \n",
       "            optimizer.zero_grad()\n",
       "            \n",
       "            # Forward pass\n",
       "            mel_spec = model(lyrics)\n",
       "            \n",
       "            # Calculate loss (L1 loss between predicted and target mel spectrograms)\n",
       "            loss = F.l1_loss(mel_spec, audio)\n",
       "            \n",
       "            # Backward pass\n",
       "            loss.backward()\n",
       "            optimizer.step()\n",
       "            \n",
       "            total_loss += loss.item()\n",
       "            progress_bar.set_postfix({'loss': loss.item()})\n",
       "        \n",
       "        avg_loss = total_loss / len(train_loader)\n",
       "        print(f'Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}')"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Initialize models\n",
       "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
       "tacotron2 = Tacotron2(vocab_size=tokenizer.vocab_size).to(device)\n",
       "wavenet = WaveNet().to(device)\n",
       "\n",
       "# Initialize optimizer\n",
       "optimizer = torch.optim.Adam(list(tacotron2.parameters()) + list(wavenet.parameters()), lr=0.001)\n",
       "\n",
       "# Create dataset and dataloader\n",
       "# TODO: Implement dataset creation with actual audio files\n",
       "dataset = RapVoiceDataset(audio_dir='data/audio', lyrics_file='data/lyrics.txt', tokenizer=tokenizer)\n",
       "train_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
       "\n",
       "# Train the model\n",
       "train_voice_generator(tacotron2, train_loader, optimizer, device)\n",
       "\n",
       "# Save the trained models\n",
       "torch.save(tacotron2.state_dict(), 'checkpoints/tacotron2_model.pth')\n",
       "torch.save(wavenet.state_dict(), 'checkpoints/wavenet_model.pth')"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 2
   } 