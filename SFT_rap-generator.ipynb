{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    " \n",
    ")\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "import datasets\n",
    "import numpy as np\n",
    "import re\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "COMET_API_KEY = os.getenv(\"COMET_API_KEY\")\n",
    "os.environ[\"COMET_LOG_ASSETS\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/artaasd95/rap-lyrics-generator-llm/d6e564e1191c4988b5c3fd2254aa96ec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = comet_ml.Experiment(api_key=COMET_API_KEY, project_name='rap-lyrics-generator-llm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.log_parameters({\n",
    "    \"num_train_epochs\": 5,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    # Add any other relevant hyperparameters here\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error exporting current conda environment\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda package as an explicit file\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda information\n"
     ]
    }
   ],
   "source": [
    "rap_lyrics_train_dataset = datasets.load_dataset(\"nateraw/rap-lyrics-v2\", split='train')\n",
    "#rap_lyrics_train_dataset = rap_lyrics_train_dataset[:int(len(rap_lyrics_train_dataset)*0.7)]\n",
    "#rap_lyrics_test_dataset = datasets.load_dataset(\"nateraw/rap-lyrics-v2\", split='train')[int(len(rap_lyrics_train_dataset)*0.7):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"  # You could use a larger model like gpt2-medium for better performance\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = model.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_prompt_and_completion(example):\n",
    "    prompt = example[\"text\"]\n",
    "    completion = example[\"completion\"]\n",
    "    # You can optionally add special tokens or formatting\n",
    "    merged_text = f\"User: {prompt}\\nAssistant: {completion}\"\n",
    "    return {\"merged_text\": merged_text}\n",
    "\n",
    "merged_dataset = rap_lyrics_train_dataset.map(merge_prompt_and_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"merged_text\"], truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = merged_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\.conda\\envs\\llmgpu\\Lib\\site-packages\\trl\\trainer\\utils.py:130: UserWarning: The pad_token_id and eos_token_id values of this tokenizer are identical. If you are planning for multi-turn training, it can result in the model continuously generating questions and answers without eos token. To avoid this, set the pad_token_id to a different value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "instruction_template = \"### Human:\"\n",
    "response_template = \"### Assistant:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_sft_structure(example):\n",
    "    merged_text = example['merged_text']\n",
    "    # Split the User and Assistant text\n",
    "    user_text = merged_text.split('User: ')[1].split('\\nAssistant: ')[0].strip()\n",
    "    assistant_text = merged_text.split('\\nAssistant: ')[1].strip()\n",
    "    \n",
    "    # Format in the desired template structure\n",
    "    formatted_text = (\n",
    "        f\"### Human: {user_text}\\n\"\n",
    "        f\"### Assistant: {assistant_text}\"\n",
    "    )\n",
    "    return {'text': formatted_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e417092eec0949aebbee8551914d797b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformed_dataset = merged_dataset.map(transform_to_sft_structure, remove_columns=['text', 'num_tokens', 'completion', 'merged_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\.conda\\envs\\llmgpu\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\HP\\.conda\\envs\\llmgpu\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37fe0819a5440b6a06e1514b80bbc58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\.conda\\envs\\llmgpu\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m An experiment with the same configuration options is already running and will be reused.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6900ceb5f7f34393861b05e05a2ec378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=transformed_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    \n",
    ")\n",
    "\n",
    "sft_trainer.train()\n",
    "\n",
    "sft_trainer.save_pretrained(\"checkpoints/gpt2-sft-checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
