{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854e8bf21cba439c8be31333af29a4e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'data/eminem_lyrics_prompt_completion'\n",
    "dataset = load_from_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns('track_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c83e347fce4f2d9a14c6412eb02cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\.conda\\envs\\llmgpu\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HP\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6049f049a84a83a6b013ad4ae9e7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979812ea1ff64cb2978c220e5e7db051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f197172d6d074509a225359cdf03cc74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8307ffee76d0461d9a0508b04532c985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6244909b6f38403ebddb6201962913c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03306cfc81ea4fa38390b69f5ec828be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    packing=False,\n",
    "    output_dir='./results',  # Directory to save the model\n",
    "    logging_dir='./logs',    # Directory to save logs\n",
    "    logging_steps=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_formatting_func(example):\n",
    "    prompt = example['prompt']\n",
    "    completion = example['completion']\n",
    "    text = prompt + completion\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\.conda\\envs\\llmgpu\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d65fe8a47924e1eb46342dc286e7114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\.conda\\envs\\llmgpu\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    formatting_func=custom_formatting_func\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/artaasd95/general/ed1fdc5113304bfab3f0e81e4b84bc1f\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08347d4051e34be0b0fcdd04084444bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/963 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error exporting current conda environment\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda package as an explicit file\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda information\n",
      "c:\\Users\\HP\\.conda\\envs\\llmgpu\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 16.7378, 'grad_norm': 25.70503044128418, 'learning_rate': 4.994807892004154e-05, 'epoch': 0.03}\n",
      "{'loss': 15.5545, 'grad_norm': 17.0876522064209, 'learning_rate': 4.9480789200415374e-05, 'epoch': 0.06}\n",
      "{'loss': 14.7086, 'grad_norm': 30.798337936401367, 'learning_rate': 4.9065420560747664e-05, 'epoch': 0.09}\n",
      "{'loss': 14.7624, 'grad_norm': 56.66119384765625, 'learning_rate': 4.8546209761163035e-05, 'epoch': 0.12}\n",
      "{'loss': 15.2694, 'grad_norm': 40.317684173583984, 'learning_rate': 4.80269989615784e-05, 'epoch': 0.16}\n",
      "{'loss': 14.4289, 'grad_norm': 58.76542663574219, 'learning_rate': 4.750778816199377e-05, 'epoch': 0.19}\n",
      "{'loss': 15.6919, 'grad_norm': 38.806854248046875, 'learning_rate': 4.7040498442367604e-05, 'epoch': 0.22}\n",
      "{'loss': 14.4302, 'grad_norm': 42.680877685546875, 'learning_rate': 4.6521287642782976e-05, 'epoch': 0.25}\n",
      "{'loss': 14.7392, 'grad_norm': 30.57004737854004, 'learning_rate': 4.60539979231568e-05, 'epoch': 0.28}\n",
      "{'loss': 14.69, 'grad_norm': 46.240089416503906, 'learning_rate': 4.553478712357217e-05, 'epoch': 0.31}\n",
      "{'loss': 15.1728, 'grad_norm': 33.98115539550781, 'learning_rate': 4.501557632398754e-05, 'epoch': 0.34}\n",
      "{'loss': 13.8064, 'grad_norm': 77.24653625488281, 'learning_rate': 4.449636552440291e-05, 'epoch': 0.37}\n",
      "{'loss': 14.4119, 'grad_norm': 35.762149810791016, 'learning_rate': 4.4080996884735206e-05, 'epoch': 0.4}\n",
      "{'loss': 15.2567, 'grad_norm': 45.16105651855469, 'learning_rate': 4.356178608515058e-05, 'epoch': 0.44}\n",
      "{'loss': 15.3108, 'grad_norm': 40.4222297668457, 'learning_rate': 4.3094496365524405e-05, 'epoch': 0.47}\n",
      "{'loss': 15.8623, 'grad_norm': 67.52513122558594, 'learning_rate': 4.262720664589824e-05, 'epoch': 0.5}\n",
      "{'loss': 16.8302, 'grad_norm': 77.45941925048828, 'learning_rate': 4.21079958463136e-05, 'epoch': 0.53}\n",
      "{'loss': 17.4003, 'grad_norm': 53.20819854736328, 'learning_rate': 4.1588785046728974e-05, 'epoch': 0.56}\n",
      "{'loss': 16.5939, 'grad_norm': 50.126651763916016, 'learning_rate': 4.1069574247144345e-05, 'epoch': 0.59}\n",
      "{'loss': 17.1041, 'grad_norm': 55.65360641479492, 'learning_rate': 4.055036344755972e-05, 'epoch': 0.62}\n",
      "{'loss': 18.1345, 'grad_norm': 64.8995132446289, 'learning_rate': 4.003115264797508e-05, 'epoch': 0.65}\n",
      "{'loss': 16.7989, 'grad_norm': 72.72815704345703, 'learning_rate': 3.951194184839045e-05, 'epoch': 0.68}\n",
      "{'loss': 17.8442, 'grad_norm': 50.82898712158203, 'learning_rate': 3.8992731048805817e-05, 'epoch': 0.72}\n",
      "{'loss': 17.0263, 'grad_norm': 55.16572952270508, 'learning_rate': 3.847352024922119e-05, 'epoch': 0.75}\n",
      "{'loss': 16.4732, 'grad_norm': 120.7039566040039, 'learning_rate': 3.8006230529595015e-05, 'epoch': 0.78}\n",
      "{'loss': 17.1342, 'grad_norm': 72.6138916015625, 'learning_rate': 3.7487019730010386e-05, 'epoch': 0.81}\n",
      "{'loss': 16.8816, 'grad_norm': 58.552284240722656, 'learning_rate': 3.696780893042576e-05, 'epoch': 0.84}\n",
      "{'loss': 18.1069, 'grad_norm': 65.97297668457031, 'learning_rate': 3.644859813084112e-05, 'epoch': 0.87}\n",
      "{'loss': 18.1522, 'grad_norm': 87.73467254638672, 'learning_rate': 3.5981308411214956e-05, 'epoch': 0.9}\n",
      "{'loss': 18.3032, 'grad_norm': 86.50749969482422, 'learning_rate': 3.546209761163033e-05, 'epoch': 0.93}\n",
      "{'loss': 20.9584, 'grad_norm': 85.7043685913086, 'learning_rate': 3.494288681204569e-05, 'epoch': 0.96}\n",
      "{'loss': 19.3909, 'grad_norm': 92.11247253417969, 'learning_rate': 3.442367601246106e-05, 'epoch': 1.0}\n",
      "{'loss': 19.5012, 'grad_norm': 75.30794525146484, 'learning_rate': 3.395638629283489e-05, 'epoch': 1.03}\n",
      "{'loss': 19.2681, 'grad_norm': 81.4178466796875, 'learning_rate': 3.343717549325026e-05, 'epoch': 1.06}\n",
      "{'loss': 20.2393, 'grad_norm': 80.86119079589844, 'learning_rate': 3.291796469366563e-05, 'epoch': 1.09}\n",
      "{'loss': 19.8805, 'grad_norm': 91.78501892089844, 'learning_rate': 3.2450674974039466e-05, 'epoch': 1.12}\n",
      "{'loss': 22.0745, 'grad_norm': 81.83332061767578, 'learning_rate': 3.193146417445483e-05, 'epoch': 1.15}\n",
      "{'loss': 22.1338, 'grad_norm': 112.45891571044922, 'learning_rate': 3.14122533748702e-05, 'epoch': 1.18}\n",
      "{'loss': 23.2517, 'grad_norm': 88.12720489501953, 'learning_rate': 3.0893042575285566e-05, 'epoch': 1.21}\n",
      "{'loss': 21.6547, 'grad_norm': 83.15299987792969, 'learning_rate': 3.0373831775700934e-05, 'epoch': 1.25}\n",
      "{'loss': 20.986, 'grad_norm': 85.73030090332031, 'learning_rate': 2.9854620976116305e-05, 'epoch': 1.28}\n",
      "{'loss': 21.4566, 'grad_norm': 89.55269622802734, 'learning_rate': 2.9335410176531676e-05, 'epoch': 1.31}\n",
      "{'loss': 23.4361, 'grad_norm': 96.8296127319336, 'learning_rate': 2.881619937694704e-05, 'epoch': 1.34}\n",
      "{'loss': 22.3603, 'grad_norm': 84.9476089477539, 'learning_rate': 2.829698857736241e-05, 'epoch': 1.37}\n",
      "{'loss': 19.393, 'grad_norm': 74.56321716308594, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.4}\n",
      "{'loss': 20.9602, 'grad_norm': 71.3188705444336, 'learning_rate': 2.7258566978193147e-05, 'epoch': 1.43}\n",
      "{'loss': 21.7391, 'grad_norm': 74.13896179199219, 'learning_rate': 2.6739356178608515e-05, 'epoch': 1.46}\n",
      "{'loss': 22.0702, 'grad_norm': 83.092529296875, 'learning_rate': 2.6220145379023886e-05, 'epoch': 1.49}\n",
      "{'loss': 21.5748, 'grad_norm': 86.45172119140625, 'learning_rate': 2.570093457943925e-05, 'epoch': 1.53}\n",
      "{'loss': 20.4813, 'grad_norm': 79.81652069091797, 'learning_rate': 2.518172377985462e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\.conda\\envs\\llmgpu\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 20.7324, 'grad_norm': 87.36784362792969, 'learning_rate': 2.4662512980269992e-05, 'epoch': 1.59}\n",
      "{'loss': 21.8107, 'grad_norm': 87.93720245361328, 'learning_rate': 2.414330218068536e-05, 'epoch': 1.62}\n",
      "{'loss': 22.6019, 'grad_norm': 71.55953216552734, 'learning_rate': 2.367601246105919e-05, 'epoch': 1.65}\n",
      "{'loss': 22.1661, 'grad_norm': 112.86392974853516, 'learning_rate': 2.315680166147456e-05, 'epoch': 1.68}\n",
      "{'loss': 22.4631, 'grad_norm': 81.55806732177734, 'learning_rate': 2.263759086188993e-05, 'epoch': 1.71}\n",
      "{'loss': 22.4102, 'grad_norm': 78.8140869140625, 'learning_rate': 2.2118380062305298e-05, 'epoch': 1.74}\n",
      "{'loss': 22.2649, 'grad_norm': 78.72042846679688, 'learning_rate': 2.1599169262720665e-05, 'epoch': 1.77}\n",
      "{'loss': 20.6629, 'grad_norm': 77.41638946533203, 'learning_rate': 2.1079958463136033e-05, 'epoch': 1.81}\n",
      "{'loss': 21.2536, 'grad_norm': 76.36277770996094, 'learning_rate': 2.05607476635514e-05, 'epoch': 1.84}\n",
      "{'loss': 22.9393, 'grad_norm': 84.59651184082031, 'learning_rate': 2.0041536863966772e-05, 'epoch': 1.87}\n",
      "{'loss': 22.1597, 'grad_norm': 72.11550903320312, 'learning_rate': 1.952232606438214e-05, 'epoch': 1.9}\n",
      "{'loss': 22.6839, 'grad_norm': 81.9189682006836, 'learning_rate': 1.9003115264797507e-05, 'epoch': 1.93}\n",
      "{'loss': 22.6104, 'grad_norm': 71.73800659179688, 'learning_rate': 1.848390446521288e-05, 'epoch': 1.96}\n",
      "{'loss': 21.9537, 'grad_norm': 77.44654083251953, 'learning_rate': 1.7964693665628246e-05, 'epoch': 1.99}\n",
      "{'loss': 22.6523, 'grad_norm': 77.10740661621094, 'learning_rate': 1.7445482866043614e-05, 'epoch': 2.02}\n",
      "{'loss': 21.2595, 'grad_norm': 68.63525390625, 'learning_rate': 1.6926272066458985e-05, 'epoch': 2.05}\n",
      "{'loss': 22.3648, 'grad_norm': 71.10057067871094, 'learning_rate': 1.6407061266874353e-05, 'epoch': 2.09}\n",
      "{'loss': 21.5023, 'grad_norm': 65.37609100341797, 'learning_rate': 1.588785046728972e-05, 'epoch': 2.12}\n",
      "{'loss': 21.4039, 'grad_norm': 73.98566436767578, 'learning_rate': 1.536863966770509e-05, 'epoch': 2.15}\n",
      "{'loss': 23.4106, 'grad_norm': 69.62877655029297, 'learning_rate': 1.4849428868120458e-05, 'epoch': 2.18}\n",
      "{'loss': 22.2712, 'grad_norm': 91.93383026123047, 'learning_rate': 1.4330218068535826e-05, 'epoch': 2.21}\n",
      "{'loss': 22.4624, 'grad_norm': 76.86775970458984, 'learning_rate': 1.3811007268951195e-05, 'epoch': 2.24}\n",
      "{'loss': 23.2976, 'grad_norm': 75.04542541503906, 'learning_rate': 1.3291796469366563e-05, 'epoch': 2.27}\n",
      "{'loss': 20.8527, 'grad_norm': 64.2414321899414, 'learning_rate': 1.277258566978193e-05, 'epoch': 2.3}\n",
      "{'loss': 20.4563, 'grad_norm': 67.28314208984375, 'learning_rate': 1.2253374870197302e-05, 'epoch': 2.33}\n",
      "{'loss': 22.1946, 'grad_norm': 90.11141967773438, 'learning_rate': 1.1734164070612668e-05, 'epoch': 2.37}\n",
      "{'loss': 21.1858, 'grad_norm': 75.42062377929688, 'learning_rate': 1.1214953271028037e-05, 'epoch': 2.4}\n",
      "{'loss': 21.9588, 'grad_norm': 75.89128112792969, 'learning_rate': 1.0695742471443407e-05, 'epoch': 2.43}\n",
      "{'loss': 22.7133, 'grad_norm': 62.26254653930664, 'learning_rate': 1.0176531671858776e-05, 'epoch': 2.46}\n",
      "{'loss': 21.2582, 'grad_norm': 82.96214294433594, 'learning_rate': 9.657320872274144e-06, 'epoch': 2.49}\n",
      "{'loss': 19.8404, 'grad_norm': 66.37129974365234, 'learning_rate': 9.138110072689512e-06, 'epoch': 2.52}\n",
      "{'loss': 22.2949, 'grad_norm': 63.55549240112305, 'learning_rate': 8.618899273104881e-06, 'epoch': 2.55}\n",
      "{'loss': 21.262, 'grad_norm': 91.8503189086914, 'learning_rate': 8.099688473520249e-06, 'epoch': 2.58}\n",
      "{'loss': 21.4915, 'grad_norm': 74.75949096679688, 'learning_rate': 7.5804776739356185e-06, 'epoch': 2.61}\n",
      "{'loss': 22.4014, 'grad_norm': 67.28801727294922, 'learning_rate': 7.061266874350987e-06, 'epoch': 2.65}\n",
      "{'loss': 21.0073, 'grad_norm': 70.62144470214844, 'learning_rate': 6.542056074766355e-06, 'epoch': 2.68}\n",
      "{'loss': 20.4117, 'grad_norm': 64.54158020019531, 'learning_rate': 6.0228452751817235e-06, 'epoch': 2.71}\n",
      "{'loss': 20.9276, 'grad_norm': 59.12961959838867, 'learning_rate': 5.503634475597093e-06, 'epoch': 2.74}\n",
      "{'loss': 22.7077, 'grad_norm': 64.21553039550781, 'learning_rate': 4.9844236760124615e-06, 'epoch': 2.77}\n",
      "{'loss': 22.483, 'grad_norm': 66.21467590332031, 'learning_rate': 4.46521287642783e-06, 'epoch': 2.8}\n",
      "{'loss': 21.9748, 'grad_norm': 60.7815055847168, 'learning_rate': 3.946002076843199e-06, 'epoch': 2.83}\n",
      "{'loss': 21.3282, 'grad_norm': 69.25292205810547, 'learning_rate': 3.426791277258567e-06, 'epoch': 2.86}\n",
      "{'loss': 22.5039, 'grad_norm': 68.0978012084961, 'learning_rate': 2.907580477673936e-06, 'epoch': 2.89}\n",
      "{'loss': 21.9881, 'grad_norm': 64.92350006103516, 'learning_rate': 2.3883696780893046e-06, 'epoch': 2.93}\n",
      "{'loss': 19.9508, 'grad_norm': 68.15965270996094, 'learning_rate': 1.8691588785046728e-06, 'epoch': 2.96}\n",
      "{'loss': 21.697, 'grad_norm': 72.65206909179688, 'learning_rate': 1.3499480789200416e-06, 'epoch': 2.99}\n",
      "{'train_runtime': 2488.5126, 'train_samples_per_second': 3.098, 'train_steps_per_second': 0.387, 'train_loss': 19.908848145545335, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model('./trained_model')\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate Rap lyrics like this lyrics:\n",
      "Look, I was gonna go easy on you not to hurt your feelings\n",
      "But I'm only going to get this one chance \n",
      "Dick I got a shit my way with that\n",
      "Thit dont the me I got to try but so try em?\n",
      "You are when I cant come this up your motherfucker?\n",
      "You dont even know you dont just leave my motherfuckers\n",
      "I gotta say you dont even look up in the up this, I got out, they are never to get in a big time and me Im like, we want the em to fuck the man, fuck a fuck I see that, I know shit, you dont think you still get your shit right in the man, I say you see my way but they dont get my man, you wont know what I go and thats with a bitch, it really got up but it dont feel in more and shit it dont know what you think you got a boy\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model and tokenizer\n",
    "model_path = './trained_model'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Generate Rap Lyrics\n",
    "prompt = \"generate Rap lyrics like this lyrics:\\nLook, I was gonna go easy on you not to hurt your feelings\\nBut I'm only going to get this one chance \"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_length=200, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
