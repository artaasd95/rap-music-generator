{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# DPO Training for Artist Style Lyrics Generation\n",
       "\n",
       "This notebook implements Direct Preference Optimization (DPO) to fine-tune the rap lyrics generator to better match specific artist styles."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.nn.functional as F\n",
       "from torch.utils.data import Dataset, DataLoader\n",
       "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
       "from datasets import load_dataset\n",
       "import numpy as np\n",
       "from tqdm import tqdm\n",
       "import os"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load the pre-trained model\n",
       "model_path = './trained_model'\n",
       "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
       "model = AutoModelForCausalLM.from_pretrained(model_path)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "class RapStyleDataset(Dataset):\n",
       "    def __init__(self, dataset_path, tokenizer, max_length=512):\n",
       "        self.tokenizer = tokenizer\n",
       "        self.max_length = max_length\n",
       "        \n",
       "        # Load the dataset\n",
       "        self.dataset = load_dataset('json', data_files=dataset_path)['train']\n",
       "        \n",
       "    def __len__(self):\n",
       "        return len(self.dataset)\n",
       "    \n",
       "    def __getitem__(self, idx):\n",
       "        item = self.dataset[idx]\n",
       "        \n",
       "        # Get the preferred and non-preferred completions\n",
       "        preferred = item['preferred']\n",
       "        non_preferred = item['non_preferred']\n",
       "        \n",
       "        # Tokenize the texts\n",
       "        preferred_tokens = self.tokenizer(\n",
       "            preferred,\n",
       "            truncation=True,\n",
       "            max_length=self.max_length,\n",
       "            padding='max_length',\n",
       "            return_tensors='pt'\n",
       "        )\n",
       "        \n",
       "        non_preferred_tokens = self.tokenizer(\n",
       "            non_preferred,\n",
       "            truncation=True,\n",
       "            max_length=self.max_length,\n",
       "            padding='max_length',\n",
       "            return_tensors='pt'\n",
       "        )\n",
       "        \n",
       "        return {\n",
       "            'preferred_input_ids': preferred_tokens['input_ids'].squeeze(),\n",
       "            'preferred_attention_mask': preferred_tokens['attention_mask'].squeeze(),\n",
       "            'non_preferred_input_ids': non_preferred_tokens['input_ids'].squeeze(),\n",
       "            'non_preferred_attention_mask': non_preferred_tokens['attention_mask'].squeeze()\n",
       "        }"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "class DPOTrainer:\n",
       "    def __init__(self, model, tokenizer, beta=0.1):\n",
       "        self.model = model\n",
       "        self.tokenizer = tokenizer\n",
       "        self.beta = beta\n",
       "        \n",
       "    def compute_loss(self, preferred_logits, non_preferred_logits, preferred_mask, non_preferred_mask):\n",
       "        # Compute policy loss\n",
       "        preferred_log_probs = F.log_softmax(preferred_logits, dim=-1)\n",
       "        non_preferred_log_probs = F.log_softmax(non_preferred_logits, dim=-1)\n",
       "        \n",
       "        # Mask out padding tokens\n",
       "        preferred_log_probs = (preferred_log_probs * preferred_mask.unsqueeze(-1)).sum(dim=1) / preferred_mask.sum(dim=1).unsqueeze(-1)\n",
       "        non_preferred_log_probs = (non_preferred_log_probs * non_preferred_mask.unsqueeze(-1)).sum(dim=1) / non_preferred_mask.sum(dim=1).unsqueeze(-1)\n",
       "        \n",
       "        # Compute DPO loss\n",
       "        loss = -torch.log(torch.sigmoid(self.beta * (preferred_log_probs - non_preferred_log_probs)))\n",
       "        \n",
       "        return loss.mean()\n",
       "    \n",
       "    def train(self, train_loader, optimizer, device, epochs=3):\n",
       "        self.model.train()\n",
       "        \n",
       "        for epoch in range(epochs):\n",
       "            total_loss = 0\n",
       "            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
       "            \n",
       "            for batch in progress_bar:\n",
       "                # Move batch to device\n",
       "                preferred_input_ids = batch['preferred_input_ids'].to(device)\n",
       "                preferred_attention_mask = batch['preferred_attention_mask'].to(device)\n",
       "                non_preferred_input_ids = batch['non_preferred_input_ids'].to(device)\n",
       "                non_preferred_attention_mask = batch['non_preferred_attention_mask'].to(device)\n",
       "                \n",
       "                optimizer.zero_grad()\n",
       "                \n",
       "                # Forward pass for preferred completions\n",
       "                preferred_outputs = self.model(\n",
       "                    input_ids=preferred_input_ids,\n",
       "                    attention_mask=preferred_attention_mask\n",
       "                )\n",
       "                \n",
       "                # Forward pass for non-preferred completions\n",
       "                non_preferred_outputs = self.model(\n",
       "                    input_ids=non_preferred_input_ids,\n",
       "                    attention_mask=non_preferred_attention_mask\n",
       "                )\n",
       "                \n",
       "                # Compute loss\n",
       "                loss = self.compute_loss(\n",
       "                    preferred_outputs.logits,\n",
       "                    non_preferred_outputs.logits,\n",
       "                    preferred_attention_mask,\n",
       "                    non_preferred_attention_mask\n",
       "                )\n",
       "                \n",
       "                # Backward pass\n",
       "                loss.backward()\n",
       "                optimizer.step()\n",
       "                \n",
       "                total_loss += loss.item()\n",
       "                progress_bar.set_postfix({'loss': loss.item()})\n",
       "            \n",
       "            avg_loss = total_loss / len(train_loader)\n",
       "            print(f'Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}')"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create dataset and dataloader\n",
       "dataset = RapStyleDataset('data/rap_style_preferences.json', tokenizer)\n",
       "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
       "\n",
       "# Initialize DPO trainer\n",
       "dpo_trainer = DPOTrainer(model, tokenizer)\n",
       "\n",
       "# Initialize optimizer\n",
       "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
       "\n",
       "# Train the model\n",
       "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
       "model = model.to(device)\n",
       "dpo_trainer.train(train_loader, optimizer, device)\n",
       "\n",
       "# Save the trained model\n",
       "model.save_pretrained('checkpoints/dpo_trained_model')\n",
       "tokenizer.save_pretrained('checkpoints/dpo_trained_model')"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 2
   } 